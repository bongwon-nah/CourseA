{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e33cc00-0679-4eb4-97e9-e8755a16665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 GPU 메모리 사용량: 5895.55 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Qwen2.5 모델과 토크나이저 로드\n",
    "model_name = \"../Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map='cuda:0')\n",
    "                                             # torch_dtype=torch.bfloat16)\n",
    "initial_memory = torch.cuda.memory_allocated()\n",
    "print(f\"초기 GPU 메모리 사용량: {initial_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b27684-89a8-4ea4-9625-6bc036359f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 기본 정보 ===\n",
      "모델 이름: ../Qwen2.5-1.5B-Instruct\n",
      "입력 토큰 수: 21\n",
      "모델 구조: Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 샘플 입력 텍스트\n",
    "sample_text = \"안녕하세요! Qwen2.5 모델의 구조를 분석해보겠습니다.\"\n",
    "\n",
    "# 입력 토큰화\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "inputs = {k:v.to('cuda:0') for k, v in inputs.items()}\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "print(\"=== 모델 기본 정보 ===\")\n",
    "print(f\"모델 이름: {model_name}\")\n",
    "print(f\"입력 토큰 수: {input_ids.shape[1]}\")\n",
    "print(f\"모델 구조: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6865d4-5299-4476-a55f-e0c4a1986b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.utils.hooks.RemovableHandle object at 0x7f161c2c18d0>, <torch.utils.hooks.RemovableHandle object at 0x7f16107cedd0>, <torch.utils.hooks.RemovableHandle object at 0x7f16319c75b0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c87c0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8880>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca1d0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca170>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8160>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8340>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c82e0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8280>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8220>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8100>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c80a0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8040>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c83a0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8490>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c86d0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8e50>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4c8df0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4cab60>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca9e0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca950>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca8f0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca890>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca830>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca7d0>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca770>, <torch.utils.hooks.RemovableHandle object at 0x7f14da4ca710>]\n"
     ]
    }
   ],
   "source": [
    "# 각 레이어의 출력을 저장하기 위한 훅\n",
    "layer_outputs = []\n",
    "layer_memory_usage = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    current_memory = torch.cuda.memory_allocated()\n",
    "    layer_memory_usage.append(current_memory)\n",
    "    layer_outputs.append(output[0].detach())\n",
    "\n",
    "# 각 레이어에 훅 등록\n",
    "hooks = []\n",
    "hooks.append(model.model.embed_tokens.register_forward_hook(hook_fn))\n",
    "for layer in model.model.layers:\n",
    "    hooks.append(layer.register_forward_hook(hook_fn))\n",
    "print(hooks)\n",
    "\n",
    "# 모델 실행\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# 훅 제거 (메모리 누수 방지지)\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a65ae6-e60b-46d0-9a59-555ff1a63df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 레이어별 출력 및 메모리 사용량 분석 ===\n",
      "\n",
      "Embedding layer 분석:\n",
      "출력 텐서 크기: torch.Size([21, 1536])\n",
      "GPU 메모리 사용량: 5895.68 MB\n",
      "\n",
      "레이어 1 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5904.11 MB\n",
      "이전 레이어 대비 메모리 증가량: 8.44 MB\n",
      "\n",
      "레이어 2 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5904.28 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 3 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5904.44 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 4 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5904.60 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 5 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5904.77 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 6 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5904.93 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 7 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5905.10 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 8 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5905.26 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 9 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5905.42 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 10 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5905.59 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 11 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5905.75 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 12 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5905.92 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 13 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5906.08 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 14 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5906.25 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 15 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5906.41 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 16 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5906.57 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 17 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5906.74 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 18 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5906.90 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 19 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5907.07 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 20 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5907.23 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 21 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5907.39 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 22 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5907.56 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 23 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5907.72 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 24 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5907.89 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 25 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5908.05 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 26 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5908.21 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 27 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5908.38 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n",
      "\n",
      "레이어 28 분석:\n",
      "출력 텐서 크기: torch.Size([1, 21, 1536])\n",
      "GPU 메모리 사용량: 5908.54 MB\n",
      "이전 레이어 대비 메모리 증가량: 0.16 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 레이어별 출력 및 메모리 사용량 분석 ===\")\n",
    "for i, (output, memory) in enumerate(zip(layer_outputs, layer_memory_usage)):\n",
    "    if i == 0:\n",
    "        print(f\"\\nEmbedding layer 분석:\")\n",
    "    else:\n",
    "        print(f\"\\n레이어 {i} 분석:\")\n",
    "    print(f\"출력 텐서 크기: {output.shape}\")\n",
    "    print(f\"GPU 메모리 사용량: {memory / 1024**2:.2f} MB\")\n",
    "    if i > 0:\n",
    "        memory_diff = (memory - layer_memory_usage[i-1]) / 1024**2\n",
    "        print(f\"이전 레이어 대비 메모리 증가량: {memory_diff:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f771e995-c1db-4675-be2e-7d912d14c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최종 출력 분석 ===\n",
      "로짓 텐서 크기: torch.Size([1, 21, 151936])\n",
      "다음 토큰 예측 확률: 0.1686\n",
      "예측된 다음 토큰:  Q\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 최종 출력 분석\n",
    "print(\"\\n=== 최종 출력 분석 ===\")\n",
    "logits = outputs.logits\n",
    "print(f\"로짓 텐서 크기: {logits.shape}\")\n",
    "print(f\"다음 토큰 예측 확률: {torch.softmax(logits[0, -1], dim=-1).max().item():.4f}\")\n",
    "\n",
    "# 다음 토큰 예측\n",
    "next_token = torch.argmax(logits[0, -1])\n",
    "print(f\"예측된 다음 토큰: {tokenizer.decode(next_token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b044a-d098-48fc-a52b-512ca1e1ac24",
   "metadata": {},
   "source": [
    "# logit을 이용하여 문장을 생성해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d36da0-ff58-4cd2-9637-1cf4b575b649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메모리 사용량: 121.42 MB\n",
      "메모리 사용량: 6.12 MB\n",
      "메모리 사용량: 6.12 MB\n",
      "메모리 사용량: 6.12 MB\n",
      "메모리 사용량: 6.12 MB\n",
      "메모리 사용량: 6.12 MB\n",
      "메모리 사용량: 6.12 MB\n",
      "메모리 사용량: 6.56 MB\n",
      "메모리 사용량: 6.51 MB\n",
      "메모리 사용량: 5.59 MB\n",
      "메모리 사용량: 14.45 MB\n",
      "메모리 사용량: -2.13 MB\n",
      "메모리 사용량: 7.96 MB\n",
      "메모리 사용량: 5.00 MB\n",
      "메모리 사용량: 16.95 MB\n"
     ]
    }
   ],
   "source": [
    "# 샘플 입력 텍스트\n",
    "sample_text = \"안녕하세요! Qwen2.5 모델의 구조를 분석해보겠습니다.\"\n",
    "current_memory = torch.cuda.memory_allocated()\n",
    "\"\"\"\n",
    "문장을 생성하는 코드 작성\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aecc67d-db59-416b-a226-1f40873626ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 메모리 사용량: 244.18 MB\n",
      "안녕하세요! Qwen2.5 모델의 구조를 분석해보겠습니다. Qwen2.5 모델은 1000개의 �\n"
     ]
    }
   ],
   "source": [
    "memory_diff = (memory - initial_memory) / 1024**2\n",
    "print(f\"총 메모리 사용량: {memory_diff:.2f} MB\")\n",
    "print(f\"{sample_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4b7a1-918b-4bbe-ae48-0ef0d7dd59dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
