{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8982b412-39ac-48ed-8ec8-71489ee617d9",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aeab26f-bb48-4725-bc3d-804392e2899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 토크나이저 기본 정보 ===\n",
      "토크나이저 클래스: Qwen2TokenizerFast\n",
      "어휘 크기: 151643\n",
      "특수 토큰: {'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Qwen2.5 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "# 토크나이저 기본 정보 출력\n",
    "print(\"=== 토크나이저 기본 정보 ===\")\n",
    "print(f\"토크나이저 클래스: {type(tokenizer).__name__}\")\n",
    "print(f\"어휘 크기: {tokenizer.vocab_size}\")\n",
    "print(f\"특수 토큰: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee354a60-76c0-4d77-b00b-4342dfa33611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 토크나이징 예시 ===\n",
      "원본 텍스트: 안녕하세요! Qwen2.5 모델의 토크나이저를 분석해보겠습니다.\n",
      "토큰화 결과: ['ìķĪ', 'ëħķ', 'íķĺìĦ¸ìļĶ', '!', 'ĠQ', 'wen', '2', '.', '5', 'Ġëª¨', 'ëį¸', 'ìĿĺ', 'Ġí', 'Ĩ', 'łí', 'ģ', '¬', 'ëĤĺ', 'ìĿ´', 'ìłĢ', 'ë¥¼', 'Ġë¶Ħ', 'ìĦĿ', 'íķ´', 'ë³´', 'ê²łìĬµëĭĪëĭ¤', '.']\n",
      "토큰 ID: [126246, 144370, 91145, 0, 1207, 16948, 17, 13, 20, 54070, 142713, 20401, 10764, 228, 57160, 223, 105, 60315, 12802, 126781, 18411, 128618, 129150, 33883, 41671, 127463, 13]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 텍스트로 토크나이징 예시\n",
    "sample_text = \"안녕하세요! Qwen2.5 모델의 토크나이저를 분석해보겠습니다.\"\n",
    "# sample_text = \"Hello! Let us analyze qwen 2.5 model.\"\n",
    "\n",
    "print(\"\\n=== 토크나이징 예시 ===\")\n",
    "print(f\"원본 텍스트: {sample_text}\")\n",
    "\n",
    "# 토큰화\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"토큰화 결과: {tokens}\")\n",
    "\n",
    "# 토큰 ID로 변환\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "print(f\"토큰 ID: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947f63b1-46bc-4cf8-937b-98a9dd21a7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩 결과: 안녕하세요! Qwen2.5 모델의 토크나이저를 분석해보겠습니다.\n",
      "\n",
      "=== 토크나이저 주요 메서드 설명 ===\n",
      "1. tokenize(): 텍스트를 토큰 리스트로 분해\n",
      "2. encode(): 텍스트를 토큰 ID 리스트로 변환\n",
      "3. decode(): 토큰 ID 리스트를 텍스트로 변환\n"
     ]
    }
   ],
   "source": [
    "# 디코딩\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"디코딩 결과: {decoded_text}\")\n",
    "\n",
    "# 토크나이저의 주요 메서드 설명\n",
    "print(\"\\n=== 토크나이저 주요 메서드 설명 ===\")\n",
    "print(\"1. tokenize(): 텍스트를 토큰 리스트로 분해\")\n",
    "print(\"2. encode(): 텍스트를 토큰 ID 리스트로 변환\")\n",
    "print(\"3. decode(): 토큰 ID 리스트를 텍스트로 변환\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d118557-7613-4e00-8935-5c1fe10db672",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m(sample_text))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(tokenizer(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f442225-c008-43b4-b8df-43db6a1865a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2c845-c4e6-4f77-b526-82ac173d745d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914d110-4568-49f2-8c44-ecd191611853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
