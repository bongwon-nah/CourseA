{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bda1572-c854-428f-96c5-e57a69926fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at ../bge-m3 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bi-Encoder 구조 ===\n",
      "모델 타입: <class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "토크나이저: <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n",
      "모델: <class 'sentence_transformers.models.Transformer.Transformer'>\n",
      "\n",
      "=== Cross-Encoder 구조 ===\n",
      "모델 타입: <class 'sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder'>\n",
      "토크나이저: <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n",
      "모델: <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "# Bi-encoder 모델 로드\n",
    "bi_encoder = SentenceTransformer('../bge-m3')\n",
    "\n",
    "# Cross-encoder 모델 로드\n",
    "cross_encoder = CrossEncoder('../bge-m3')\n",
    "\n",
    "print(\"\\n=== Bi-Encoder 구조 ===\")\n",
    "print(f\"모델 타입: {type(bi_encoder)}\")\n",
    "print(f\"토크나이저: {type(bi_encoder.tokenizer)}\")\n",
    "print(f\"모델: {type(bi_encoder._modules['0'])}\")\n",
    "\n",
    "print(\"\\n=== Cross-Encoder 구조 ===\")\n",
    "print(f\"모델 타입: {type(cross_encoder)}\")\n",
    "print(f\"토크나이저: {type(cross_encoder.tokenizer)}\")\n",
    "print(f\"모델: {type(cross_encoder.model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6039785-18ab-4b8b-bafe-4781c0b5b63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "=====================\n",
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): XLMRobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bi_encoder)\n",
    "print(\"=====================\")\n",
    "print(cross_encoder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6910b74-0ba1-49fd-a925-5e98801b6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bi-Encoder 예시 ===\n",
      "임베딩 차원: (2, 1024)\n",
      "유사도: 0.7982245683670044\n",
      "\n",
      "=== Cross-Encoder 예시 ===\n",
      "유사도 점수: [0.55364424]\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장\n",
    "sentences = [\"안녕하세요\", \"반갑습니다\"]\n",
    "\n",
    "# Bi-encoder 예시\n",
    "print(\"\\n=== Bi-Encoder 예시 ===\")\n",
    "embeddings = bi_encoder.encode(sentences)\n",
    "print(f\"임베딩 차원: {embeddings.shape}\")\n",
    "similarity = np.dot(embeddings[0], embeddings[1])\n",
    "print(f\"유사도: {similarity}\")\n",
    "\n",
    "# Cross-encoder 예시\n",
    "print(\"\\n=== Cross-Encoder 예시 ===\")\n",
    "scores = cross_encoder.predict([(sentences[0], sentences[1])])\n",
    "print(f\"유사도 점수: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3643c3-81ec-4b1a-935d-e960b9c3b884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
