{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6396d23c-acd8-47b8-b5e1-475fb85cb196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19680/4209238674.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# BGE M3 임베딩 모델 설정\n",
    "embedding_model_name = \"../bge-m3\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472558ac-02b6-4799-99d4-c64d02e15088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# Qwen2.5-1.5B-Instruct 모델 설정\n",
    "model_name = \"../Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea99e18-712e-4796-a2fa-cb4f92ca1182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 텍스트 생성 파이프라인 설정\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "def create_rag_pipeline(pdf_path):\n",
    "    # PDF 문서 로드\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # 텍스트 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    \n",
    "    # RAG 체인 생성\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "def query_rag_pipeline(qa_chain, question):\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return {\n",
    "        \"answer\": result[\"result\"],\n",
    "        \"source_documents\": result[\"source_documents\"]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315b7fb9-fb9c-4ea6-9820-87630ea96ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "adaptation of BERT and performance on downstream\n",
      "document classification: Insights from social media.\n",
      "In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2021, pages 2400–2412, Punta\n",
      "Cana, Dominican Republic. Association for Compu-\n",
      "tational Linguistics.\n",
      "Devendra Singh Sachan, Siva Reddy, William L. Hamil-\n",
      "ton, Chris Dyer, and Dani Yogatama. 2021. End-to-\n",
      "end training of multi-document reader and retriever\n",
      "for open-domain question answering. In Advances\n",
      "in Neural Information Processing Systems 34: An-\n",
      "nual Conference on Neural Information Processing\n",
      "Systems 2021, NeurIPS 2021, December 6-14, 2021,\n",
      "virtual, pages 25968–25981.\n",
      "Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\n",
      "Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\n",
      "Cancedda, and Thomas Scialom. 2023. Toolformer:\n",
      "\n",
      "et al., 2019).\n",
      "R (st, at) =Rlm(ˆ˜x, y) − βKL (πθ∥π0) . (4)\n",
      "The final loss function is composed of policy loss\n",
      "and value loss.\n",
      "Lθ = − 1\n",
      "|S| T\n",
      "X\n",
      "τ∈S\n",
      "TX\n",
      "t=0\n",
      "min(kt,θAθ′\n",
      ", clip Aθ′\n",
      "),\n",
      "Lϕ = 1\n",
      "|S| T\n",
      "X\n",
      "τ∈S\n",
      "TX\n",
      "t=0\n",
      "(Vϕ (st) − Rt)2 ,\n",
      "Lppo = Lθ + λvLϕ.\n",
      "(5)\n",
      "Here, S denotes the sampled set, and T is for step\n",
      "numbers.\n",
      "4 Implementation\n",
      "Rewriter For the frozen pipeline in §3.1, we\n",
      "prompt an LLM to rewrite the query with few-shot\n",
      "in-context learning (Brown et al., 2020; Min et al.,\n",
      "2022). Our prompt follows the formulation of [in-\n",
      "struction, demonstrations, input], where the input\n",
      "is x. The instruction is straightforward and demon-\n",
      "strations are 1-3 random examples from training\n",
      "sets and are kept constant across all runs, mainly\n",
      "for the task-specific output format illustration, i.e.,\n",
      "\n",
      "The work environment for the employees\n",
      "       at the plant\n",
      "Q1: What would community planners be most \n",
      "       concerned  with before allowing a car-\n",
      "       manufacturing factory to be built?\n",
      "Q2: 2000 movie \"All Star\" song\n",
      "Q0: What 2000 movie does the song \"All Star\"\n",
      "       appear in?\n",
      "✅ ✅\n",
      "✅ ✅\n",
      "✅ ✅\n",
      "✅ ✅\n",
      "❌ ❌\n",
      "❌ ❌\n",
      "❌ ❌\n",
      "❌ ❌\n",
      "Figure 3: Examples for intuitive illustration. Q0 denotes\n",
      "original input, Q1 is from the LLM rewriter, and Q2 is\n",
      "from the trained T5 rewriter. Hit means retriever recall\n",
      "the answer, while Correct is for the reader output.\n",
      "while query rewriting makes progress on both set-\n",
      "tings. We also observed that the improvement in\n",
      "the hit rate of the retriever is more significant than\n",
      "the improvement in the reader. This is consistent\n",
      "with the findings in related search (Mallen et al.,\n",
      "\n",
      "Question: 문서에서 설명하는 주요 내용은 무엇인가요?\n",
      "Helpful Answer: ### 주요 내용 요약:\n",
      "주요 문항과 함께 이해를 돕는 실질적인 예시들이 제공됩니다. 이 예시들은 질문에 대한 답변을 찾기 위한 단계별로 이루어져 있습니다. \n",
      "\n",
      "예를 들어, 문항 Q0은 '2000년 영화 \"All Star\"'의 노래가 어떤 영화에서 나왔는지 묻습니다. 이를 해결하기 위해, 문제 제출 시에는 사용자에게 그 결과를 알려줄 수 있는 여러 가지 방법들을 제공합니다. \n",
      "\n",
      "먼저 'Q0'으로부터 시작해, 원본 입력이 'employees at the plant'이고 라벨이 확인된 것으로 표시되어 있습니다. 이외에도, 질문 Q1은 'LLM을 통해 만들어진 rewritten version', Q2는 'Trained T5를 통해 만들어진 rewritten version'입니다. 각각의 정답을 찾아내는데 도움을 주고 있음을 알 수 있습니다. 또한, 일부 예제들은 해당 질문에 대한 정확한 답변을 출력하지 못했다는 점도 언급하고 있습니다. 이러한 예시들이 일반적으로 사용되는 검색 프로세스와 관련하여 중요한 팁을 제공하므로, 이는 문서에서 가장 많이 다루는 주요 내용이라고 볼 수 있습니다. \n",
      "\n",
      "### 주요 내용 분석:\n",
      "문서에서는 주로 문서에서 특정 정보를 추적하거나 문제解答에 필요한 단계들을 자동화하는데 초점을 맞추었습니다. 특히, 질문 풀이 과정에 대한 디테일하게 구체적인 예시들을 제시하며, 이들 예시들을 통해 검색 엔진의 기능 및 작용방식을 파악할 수 있게 해줍니다.\n",
      "\n",
      "또한, 질문을 처리하는 과정 중에 다양한 유형의 답변이 생성되고 이를 활용하여 더 효율적인 답변을 제공하는 방안에 대해 논의하였습니다. 이는 질문에 따라 다양한 가능한 답변들이 생성되며, 이를 통해 검색엔진의 능력을 더욱 강화할 수 있다는 것을 보여주고 있습니다.\n",
      "\n",
      "결론적으로, 본 연구는 다양한 형태의 질문에 대응할 수 있는 고성능 문서 검색 엔진 개발에 관해 매우 중요한 성과를 내놓았다는 것입니다. 이러한 기술이 실제 사회 현장이나 공공 서비스 등 다양한 분야에서 큰 영향력을 미칠 것임을 인지해야\n",
      "\n",
      "참고 문서:\n",
      "- adaptation of BERT and performance on downstream\n",
      "document classification: Insights from social media.\n",
      "In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2021, pages 2400–2412, Punta\n",
      "C...\n",
      "- et al., 2019).\n",
      "R (st, at) =Rlm(ˆ˜x, y) − βKL (πθ∥π0) . (4)\n",
      "The final loss function is composed of policy loss\n",
      "and value loss.\n",
      "Lθ = − 1\n",
      "|S| T\n",
      "X\n",
      "τ∈S\n",
      "TX\n",
      "t=0\n",
      "min(kt,θAθ′\n",
      ", clip Aθ′\n",
      "),\n",
      "Lϕ = 1\n",
      "|S| T\n",
      "X\n",
      "τ∈S\n",
      "T...\n",
      "- The work environment for the employees\n",
      "       at the plant\n",
      "Q1: What would community planners be most \n",
      "       concerned  with before allowing a car-\n",
      "       manufacturing factory to be built?\n",
      "Q2: 2000 m...\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"sample.pdf\"\n",
    "\n",
    "# RAG 파이프라인 생성\n",
    "qa_chain = create_rag_pipeline(pdf_path)\n",
    "\n",
    "# 질문 예시\n",
    "question = \"문서에서 설명하는 주요 내용은 무엇인가요?\"\n",
    "\n",
    "# 질문에 대한 답변 생성\n",
    "result = query_rag_pipeline(qa_chain, question)\n",
    "print(\"답변:\", result[\"answer\"])\n",
    "print(\"\\n참고 문서:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.page_content[:200]}...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de651283-6d23-43d6-9597-1fe335793726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b4205-6dd8-4c9b-9dd1-2ae985dcb1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
