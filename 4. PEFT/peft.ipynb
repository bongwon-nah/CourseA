{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c301678a-9a5e-4096-b1ca-edc34ba643e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb53e1cd-eb6c-4966-b8a1-7ea46dd953de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a864d697-0657-4ac5-a8f2-8ec91ceac4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "model_name = \"../Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cuda:0',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4715e269-f2a8-495e-bcc6-ae2db6ef7959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fdefe43-fa0c-4f34-9589-ee7f53696689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PEFT 설정\n",
    "# peft_config = LoraConfig(\n",
    "#     r=32,\n",
    "#     lora_alpha=64,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# 모델 준비\n",
    "# model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f9a1f5-c326-4209-a3e0-c5c21f121e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da6f575-7c0c-4068-b9de-4cc2611971aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset load 직후:\n",
      " {'question': 'Janet의 오리는 하루에 16개의 알을 낳습니다. 그녀는 매일 아침으로 3개를 먹고, 친구들을 위해 머핀을 구울 때 4개를 사용합니다. 남은 계란은 매일 농산물 시장에서 신선한 오리 알 하나당 2달러에 판매합니다. 그녀는 매일 농산물 시장에서 얼마를 버나요?', 'answer': 18.0, 'original': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"}\n",
      "Text column 추가:\n",
      " {'messages': [{'content': 'Janet의 오리는 하루에 16개의 알을 낳습니다. 그녀는 매일 아침으로 3개를 먹고, 친구들을 위해 머핀을 구울 때 4개를 사용합니다. 남은 계란은 매일 농산물 시장에서 신선한 오리 알 하나당 2달러에 판매합니다. 그녀는 매일 농산물 시장에서 얼마를 버나요?', 'role': 'user'}, {'content': '18.0', 'role': 'assistant'}], 'text': '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nJanet의 오리는 하루에 16개의 알을 낳습니다. 그녀는 매일 아침으로 3개를 먹고, 친구들을 위해 머핀을 구울 때 4개를 사용합니다. 남은 계란은 매일 농산물 시장에서 신선한 오리 알 하나당 2달러에 판매합니다. 그녀는 매일 농산물 시장에서 얼마를 버나요?<|im_end|>\\n<|im_start|>assistant\\n18.0<|im_end|>\\n'}\n",
      "Tokenizer 통과:\n",
      " {'input_ids': [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 18315, 295, 20401, 73077, 132920, 53900, 126746, 19391, 220, 16, 21, 59761, 20401, 125214, 17877, 37195, 111, 38231, 13, 54825, 132751, 16560, 126932, 32077, 48408, 130436, 42039, 220, 18, 59761, 18411, 137767, 117, 34395, 11, 90711, 250, 88259, 129125, 130039, 137767, 116, 144355, 17877, 58777, 126893, 53618, 220, 19, 59761, 18411, 40720, 60838, 13, 129624, 33704, 94203, 129804, 33704, 126932, 32077, 5140, 228, 235, 85057, 126251, 44518, 40853, 56475, 128753, 125519, 23573, 73077, 28002, 125214, 125703, 64795, 220, 17, 129062, 60294, 19391, 140568, 129865, 60838, 13, 54825, 132751, 16560, 126932, 32077, 5140, 228, 235, 85057, 126251, 44518, 40853, 56475, 138666, 18411, 86831, 60315, 35711, 30, 151645, 198, 151644, 77091, 198, 16, 23, 13, 15, 151645, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드 및 전처리\n",
    "def dataset_to_text(data):\n",
    "    new_data = {}\n",
    "    new_data['messages'] = [{'role':'user','content':data['question']},\n",
    "                           {'role':'assistant','content':str(data['answer'])}]\n",
    "    new_data['text'] = tokenizer.apply_chat_template(new_data['messages'], tokenize=False)\n",
    "    return new_data\n",
    "\n",
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"../hrm8k_dataset.csv\")  # 실제 데이터셋으로 교체 필요\n",
    "# if int(os.envionr[\"RANK\"])==0:\n",
    "print(f\"Dataset load 직후:\\n {dataset['train'][0]}\")\n",
    "dataset = dataset.map(dataset_to_text, \n",
    "                      remove_columns=dataset['train'].column_names)\n",
    "print(f\"Text column 추가:\\n {dataset['train'][0]}\")\n",
    "tokenized_dataset = dataset.map(tokenize_text, \n",
    "                                remove_columns=dataset['train'].column_names)\n",
    "print(f\"Tokenizer 통과:\\n {tokenized_dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47b1e4d-94f7-42f6-a16e-0c907bd5f4eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2593\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2588\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2589\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2590\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2591\u001b[0m     )\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2593\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2599\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2600\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2601\u001b[0m ):\n\u001b[1;32m   2602\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2609\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2607\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;241m==\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]:\n\u001b[1;32m   2608\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m-> 2609\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2548\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2547\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2548\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    335\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    336\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 338\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:279\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m device, per_dtype_grads \u001b[38;5;129;01min\u001b[39;00m per_device_and_dtype_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grads \u001b[38;5;129;01min\u001b[39;00m per_dtype_grads\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 279\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_amp_foreach_non_finite_check_and_unscale_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_found_inf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_inv_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m per_device_found_inf\u001b[38;5;241m.\u001b[39m_per_device_tensors\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./train_results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "# # 모델 저장\n",
    "# model.save_pretrained(\"./final_model\")\n",
    "# tokenizer.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df7960-1a94-48e9-89a6-ae5ae114b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장\n",
    "# model.save_pretrained(\"./final_model\")\n",
    "# tokenizer.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b98ed7-6702-4f82-894b-253687082762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
