{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b82788a-f882-402f-b3b4-a1085ff4ea91",
   "metadata": {},
   "source": [
    "### 초기 명령어 모음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee786b-867f-463b-80c1-5d453df0cc86",
   "metadata": {},
   "source": [
    "# 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2e340-409f-497e-b8bf-9d6610a568bc",
   "metadata": {},
   "source": [
    "### Qwen2.5-1.5B 모델 다운로드\n",
    "> sudo apt install git-lfs \\\n",
    "> git clone https://huggingface.co/Qwen/Qwen2.5-1.5B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e660093-ed02-4888-b46a-ef4b157e913b",
   "metadata": {},
   "source": [
    "### 필요한 python 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd4fa0-5ab0-4eda-8ca9-8875226f3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate vllm==0.7.2 locust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88771ed4-e7f5-41de-a074-e0a1f920ace7",
   "metadata": {},
   "source": [
    "# 2. local vLLM과 transformers 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00709b72-87e1-4a03-8b3d-8f3353c31a54",
   "metadata": {},
   "source": [
    "## transformers: 모델과 토크나이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bc0de-9fb5-4889-a4af-c10d374aeee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"./models/Qwen2.5-1.5B-Instruct/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e64155-9e5a-4a8d-a478-bbba5d3fad67",
   "metadata": {},
   "source": [
    "## transformers: 기본 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455cb7c-1f21-4fe8-8299-b3810f283e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, my name is\"\n",
    "input_tokens = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "\n",
    "start_time = time()\n",
    "output_tokens = model.generate(**input_tokens, \n",
    "                               max_new_tokens=200, \n",
    "                               temperature=0.8,\n",
    "                               top_p=0.95,\n",
    "                              )\n",
    "output = tokenizer.decode(output_tokens[0])\n",
    "print(output)\n",
    "end_time = time()\n",
    "print(\"Time spent to generate:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d957e-85f5-4e42-92e3-a8b30cfc22f1",
   "metadata": {},
   "source": [
    "## transformers: 스트리밍 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399981b9-f16c-4fec-ab65-37dfc242dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Hello, my name is\"\n",
    "print(prompt, end='')\n",
    "input_tokens = tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "\n",
    "start_time = time()\n",
    "output_tokens = model.generate(**input_tokens, \n",
    "                               max_new_tokens=200, \n",
    "                               temperature=0.8,\n",
    "                               top_p=0.95,\n",
    "                               streamer=streamer)\n",
    "end_time = time()\n",
    "print(\"Time spent to generate:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a598e-fb07-474d-bc77-2cc51253ad1c",
   "metadata": {},
   "source": [
    "## local vLLM: 기본 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1eb1a-44bf-4c24-b591-c960bcce9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"./models/Qwen2.5-1.5B-Instruct/\"\n",
    "llm = LLM(model=model_path, \n",
    "          dtype='half',\n",
    "          gpu_memory_utilization=0.6,\n",
    "          enforce_eager=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ef795-e133-4c0d-b033-0c0e768424b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, my name is\"\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=200,\n",
    "    # stream=True\n",
    ")\n",
    "\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a71119-ca34-41d4-bd25-3bcb7dd48b2d",
   "metadata": {},
   "source": [
    "# 3. vLLM을 이용한 API server 띄우기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d60d29-3c0c-4987-82aa-4c7b11131a84",
   "metadata": {},
   "source": [
    "- 터미널 탭을 추가로 띄워서 실행\n",
    "- 다운로드 받은 모델로 vLLM 서빙 띄우는 명령어:\n",
    "```bash\n",
    "$ vllm serve models/Qwen2.5-1.5B-Instruct/ \\\n",
    "  --port 8006 \\\n",
    "  --dtype half \\\n",
    "  --gpu-memory-utilization 0.6 \\\n",
    "  --enforce-eager\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfc914-ac08-4c6f-a048-e9caa33ac636",
   "metadata": {},
   "source": [
    "- 입력변수 설명\n",
    "    - `--gpu-memory-utilization`: 초기에 vllm에서 잡고 가는 GPU 메모리 비율\n",
    "    - `--enforce-eager`: pytorch에서 CUDA graph 세팅 기능을 끄는 입력변수\n",
    "        - CUDA graph 세팅 시 속도↑ & 기본 GPU 메모리 사용량 증가↑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6c4b3-2857-46cb-8bcc-7c1da8aa381e",
   "metadata": {},
   "source": [
    "- 우리 환경에서 추가 실험 가능한 arguments:\n",
    "    - `--max-model-len`: 모델의 입력+출력 최대 길이 설정\n",
    "    - `--enable-prefix-caching` or `--no-enable-prefix-caching`: prefix caching의 on-off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
